{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kenny TANCHALEUNE\n",
    "\n",
    "Email: kenny.tanchaleune@gmail.com\n",
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import datefinder\n",
    "from nltk.tag.stanford import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouverture et lecture du fichier text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester mes différents programmes, j'ai créé un fichier text1 dans lequel j'ai mis un extrait d'un discours du président Emmanuel MACRON (sources: https://www.lemonde.fr/politique/article/2020/04/13/nous-tiendrons-l-integralite-du-discours-d-emmanuel-macron_6036480_823448.html )\n",
    "J'ai ensuite ajouté quelques mots spécifiques pour tester correctement certains programmes ci-dessous\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour accompagner cette phase, plusieurs innovations font l’objet de travaux avec certains de nos partenaires européens, comme une application numérique dédiée qui, sur la base du volontariat et de l’anonymat, permettra de savoir si, oui ou non, l’on s’est trouvé en contact avec une personne contaminée. Vous en avez sûrement entendu parler. Le gouvernement aura à y travailler, il ne faut négliger aucune piste, aucune innovation. Mais je souhaite qu’avant le 11 mai nos Assemblées puissent en débattre, et que les autorités compétentes puissent nous éclairer. Cette épidémie ne saurait affaiblir notre démocratie, ni mordre sur quelques libertés.\n",
      "Azerty\n",
      "Zazerty\n",
      "azertyb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file1 = open(\"text1.txt\",\"r\",encoding='utf8') \n",
    "text1 = file1.read()\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite j'ai créé un fichier text2 dans lequel j'ai mis un extrait d'un discours du président Donald TRUMP (sources: https://www.rev.com/blog/transcripts/donald-trump-speech-transcript-on-vaccine-development-for-coronavirus ). Ce deuxième texte a pour but de tester les dates et les adverbes en Anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another essential pillar of our strategy to keep America open is the development of effective treatments and vaccines as quickly as possible. Want to see if we can do that very quickly. We’re looking to, when I say quickly, we’re looking to get it by the end of the year if we can, maybe before. We’re doing tremendously well. From the earliest days of the pandemic, we have marshaled the genius of American scientists and researchers from all across government and the private sector, from academia, from everywhere to vanquish the virus, and tremendous strides have been made. Can tell you, I get to see it every day. Tremendous strides are being made. Scientists at the NIH began developing the first vaccine candidate on January 11th, think of that, within hours of the virus’s genetic code being posted online. So January 11th, most people never even heard what was going on January 11th. And we were out there trying to develop a vaccine, not even knowing what we were up against. Then my administration cut through every piece of red tape to achieve the fastest ever, by far, launch of a vaccine trial for this new virus, this very vicious virus. And I want to thank all of the doctors and scientists and researchers involved because they’ve never moved like this or never even close.\n"
     ]
    }
   ],
   "source": [
    "file2 = open(\"text2.txt\",\"r\",encoding='utf8') \n",
    "text2 = file2.read()\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program to find sequences of one upper case letter followed by lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots qui commencent par une majuscule: \n",
      "\n",
      "Pour\n",
      "Vous\n",
      "Le\n",
      "Mais\n",
      "11\n",
      "Assemblées\n",
      "Cette\n",
      "Azerty\n",
      "Zazerty\n",
      "\n",
      "\n",
      "Mots qui commencent par une majuscule: \n",
      "\n",
      "Another\n",
      "America\n",
      "Want\n",
      "We\n",
      "I\n",
      "We\n",
      "From\n",
      "American\n",
      "Can\n",
      "I\n",
      "Tremendous\n",
      "Scientists\n",
      "NIH\n",
      "January\n",
      "11th\n",
      "So\n",
      "January\n",
      "11th\n",
      "January\n",
      "11th\n",
      "And\n",
      "Then\n",
      "And\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "def upperWord(text):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    f = tokenizer.tokenize(text)\n",
    "    print(\"Mots qui commencent par une majuscule: \\n\")\n",
    "    for element in f:\n",
    "        if element[0][0].upper() in element:\n",
    "            if element[0][0].upper() is not type(float):\n",
    "                print(element)\n",
    "upperWord(text1)\n",
    "print(\"\\n\")\n",
    "upperWord(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program that matches a string that has an 'a' followed by anything, ending in 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots qui contient un a et qui se termine par un b: \n",
      "\n",
      "azertyb\n",
      "\n",
      "\n",
      "Mots qui contient un a et qui se termine par un b: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ab(text):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    f = tokenizer.tokenize(text)\n",
    "    l = []\n",
    "    print(\"Mots qui contient un a et qui se termine par un b: \\n\")\n",
    "    for element in f:\n",
    "        for n in element:\n",
    "            if n =='a' or n =='A':\n",
    "                l.append(element)\n",
    "    for element in l:\n",
    "        if element[-1] == 'b':\n",
    "            print(element)\n",
    "ab(text1)\n",
    "print(\"\\n\")\n",
    "ab(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program that matches a word containing 'z', not start or end of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots qui contient un z ni au début, ni à la fin: \n",
      "\n",
      "Azerty\n",
      "azertyb\n",
      "\n",
      "\n",
      "Mots qui contient un z ni au début, ni à la fin: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def az(text):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    f = tokenizer.tokenize(text)\n",
    "    l = []\n",
    "    print(\"Mots qui contient un z ni au début, ni à la fin: \\n\")\n",
    "    for element in f:\n",
    "        for n in element:\n",
    "            if n == 'z':\n",
    "                l.append(element)\n",
    "    for element in l:\n",
    "        if element[0][0] != 'z' and element[0][0] != 'Z':\n",
    "            if element[-1] != 'z':\n",
    "                print(element)\n",
    "az(text1)\n",
    "print(\"\\n\")\n",
    "az(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program to split a string with multiple delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delimiter pour chaques String: \n",
      "\n",
      "['Pour accompagner cette phase', 'plusieurs innovations font l’objet de travaux avec certains de nos partenaires européens', 'comme une application numérique dédiée qui', 'sur la base du volontariat et de l’anonymat', 'permettra de savoir si', 'oui ou non', 'l’on s’est trouvé en contact avec une personne contaminée. Vous en avez sûrement entendu parler. Le gouvernement aura à y travailler', 'il ne faut négliger aucune piste', 'aucune innovation. Mais je souhaite qu’avant le 11 mai nos Assemblées puissent en débattre', 'et que les autorités compétentes puissent nous éclairer. Cette épidémie ne saurait affaiblir notre démocratie', 'ni mordre sur quelques libertés.', 'Azerty', 'Zazerty', 'azertyb', '']\n",
      "\n",
      "\n",
      "Delimiter pour chaques String: \n",
      "\n",
      "['Another essential pillar of our strategy to keep America open is the development of effective treatments and vaccines as quickly as possible. Want to see if we can do that very quickly. We’re looking to', 'when I say quickly', 'we’re looking to get it by the end of the year if we can', 'maybe before. We’re doing tremendously well. From the earliest days of the pandemic', 'we have marshaled the genius of American scientists and researchers from all across government and the private sector', 'from academia', 'from everywhere to vanquish the virus', 'and tremendous strides have been made. Can tell you', 'I get to see it every day. Tremendous strides are being made. Scientists at the NIH began developing the first vaccine candidate on January 11th', 'think of that', 'within hours of the virus’s genetic code being posted online. So January 11th', 'most people never even heard what was going on January 11th. And we were out there trying to develop a vaccine', 'not even knowing what we were up against. Then my administration cut through every piece of red tape to achieve the fastest ever', 'by far', 'launch of a vaccine trial for this new virus', 'this very vicious virus. And I want to thank all of the doctors and scientists and researchers involved because they’ve never moved like this or never even close.']\n"
     ]
    }
   ],
   "source": [
    "def delimiter(text):\n",
    "    print(\"Delimiter pour chaques String: \\n\")\n",
    "    print(re.split('; |, |\\*|\\n',text))\n",
    "delimiter(text1)\n",
    "print(\"\\n\")\n",
    "delimiter(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program to find all adverbs and their positions in a given sentence (do this exercice for English adverbs and French adverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la partie en Français, j'ai utilisé StanfordPOSTagger pour trouver les adverbes car les librairies nltk existants reconnaissent seulement les mots en anglais. Cette méthode demande d'installer des fichiers supplémentaires. Pour cela, je me suis aidé de la source que j'ai trouvé le site suivant: \n",
    "https://www.le-geek.com/pos_tag-pour-les-textes-francais/\n",
    "Pour des raisons de taille je n'ai pas mis les fichiers mais ceci sont téléchargeables sur le lien suivant: https://nlp.stanford.edu/software/tagger.shtml\n",
    "Ensuite il faut mettre le chemin du jdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des adverbes en Français:  ['sûrement', 'y', 'ne', 'ne']\n",
      "\n",
      "\n",
      "Liste des adverbes en Anglais:  ['as', 'quickly', 'very', 'quickly', 'quickly', 'maybe', 'before', 'tremendously', 'well', 'everywhere', 'So', 'never', 'even', 'out', 'there', 'not', 'even', 'up', 'Then', 'ever', 'far', 'very', 'never', 'never', 'even', 'close']\n"
     ]
    }
   ],
   "source": [
    "def adverbeAnglais(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    a = nltk.pos_tag(tokens)\n",
    "    l = []\n",
    "    for element in a:\n",
    "        if element[1] == 'RB' or element[1] == 'RBS' or element[1] == 'RBR':\n",
    "            l.append(element[0])\n",
    "    print(\"Liste des adverbes en Anglais: \",l)\n",
    "\n",
    "def adverbeFrancais(text):\n",
    "    java_path = \"C:\\Program Files\\Java\\jdk1.8.0_151\"\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    root_path=\"stanford-tagger/\"\n",
    "    pos_tagger = StanfordPOSTagger(root_path + \"models/french-ud.tagger\", root_path + \"stanford-postagger.jar\",encoding='utf8')\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    a = pos_tagger.tag(tokens) \n",
    "    l = []\n",
    "    for element in a:\n",
    "        if element[1] == 'ADV':\n",
    "            l.append(element[0])\n",
    "    print(\"Liste des adverbes en Français: \",l)\n",
    "\n",
    "\n",
    "adverbeFrancais(text1)\n",
    "print(\"\\n\")\n",
    "adverbeAnglais(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program to find all Dates in a text (Both French Format and English Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des dates présent dans le texte: \n",
      "\n",
      "2020-06-11 00:00:00\n",
      "\n",
      "\n",
      "Liste des dates présent dans le texte: \n",
      "\n",
      "2020-01-11 00:00:00\n",
      "2020-01-11 00:00:00\n",
      "2020-01-11 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def findDates(text):\n",
    "    date = datefinder.find_dates(text)\n",
    "    print(\"Liste des dates présent dans le texte: \\n\")\n",
    "    for element in date:\n",
    "        print(element)\n",
    "        \n",
    "findDates(text1)\n",
    "print(\"\\n\")\n",
    "findDates(text2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
